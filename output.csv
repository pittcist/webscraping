"Datasets and BenchmarksIn 2021, NeurIPS introduced a new track, Datasets and Benchmarks. The first year of that
            track, 2021, has its own proceedings, accessible by the link below. From 2022 on, the Datasets and
            Benchmarks papers are in the main NeurIPS proceedings.Advances in
            Neural Information Processing Systems Datasets and Benchmarks, 2021Advances in Neural Information Processing Systems 37  (NeurIPS 2024)Advances in Neural Information Processing Systems 36  (NeurIPS 2023)Advances in Neural Information Processing Systems 35  (NeurIPS 2022)Advances in Neural Information Processing Systems 34  (NeurIPS 2021)Advances in Neural Information Processing Systems 33  (NeurIPS 2020)Advances in Neural Information Processing Systems 32  (NeurIPS 2019)Advances in Neural Information Processing Systems 31  (NeurIPS 2018)Advances in Neural Information Processing Systems 30  (NIPS 2017)Advances in Neural Information Processing Systems 29  (NIPS 2016)Advances in Neural Information Processing Systems 28  (NIPS 2015)Advances in Neural Information Processing Systems 27  (NIPS 2014)Advances in Neural Information Processing Systems 26  (NIPS 2013)Advances in Neural Information Processing Systems 25  (NIPS 2012)Advances in Neural Information Processing Systems 24  (NIPS 2011)Advances in Neural Information Processing Systems 23  (NIPS 2010)Advances in Neural Information Processing Systems 22  (NIPS 2009)Advances in Neural Information Processing Systems 21  (NIPS 2008)Advances in Neural Information Processing Systems 20  (NIPS 2007)Advances in Neural Information Processing Systems 19  (NIPS 2006)Advances in Neural Information Processing Systems 18  (NIPS 2005)Advances in Neural Information Processing Systems 17  (NIPS 2004)Advances in Neural Information Processing Systems 16  (NIPS 2003)Advances in Neural Information Processing Systems 15  (NIPS 2002)Advances in Neural Information Processing Systems 14  (NIPS 2001)Advances in Neural Information Processing Systems 13  (NIPS 2000)Advances in Neural Information Processing Systems 12  (NIPS 1999)Advances in Neural Information Processing Systems 11  (NIPS 1998)Advances in Neural Information Processing Systems 10  (NIPS 1997)Advances in Neural Information Processing Systems 9  (NIPS 1996)Advances in Neural Information Processing Systems 8  (NIPS 1995)Advances in Neural Information Processing Systems 7  (NIPS 1994)Advances in Neural Information Processing Systems 6  (NIPS 1993)Advances in Neural Information Processing Systems 5  (NIPS 1992)Advances in Neural Information Processing Systems 4  (NIPS 1991)Advances in Neural Information Processing Systems 3  (NIPS 1990)Advances in Neural Information Processing Systems 2  (NIPS 1989)Advances in Neural Information Processing Systems 1  (NIPS 1988)Neural Information Processing Systems 0  (NIPS 1987)"
"MicroAdam: Accurate Adaptive Optimization with Low Space Overhead and Provable ConvergencePart ofAdvances in Neural Information Processing Systems 37  (NeurIPS 2024)Main Conference TrackBibtexPaperSupplementalAuthorsIonut-Vlad Modoranu, Mher Safaryan, Grigory Malinovsky, Eldar Kurtic, Thomas Robert, Peter Richtárik, Dan AlistarhAbstractWe propose a new variant of the Adam optimizer called MicroAdam that specifically minimizes memory overheads, while maintaining theoretical convergence guarantees. We achieve this by compressing the gradient information before it is fed into the optimizer state, thereby reducing its memory footprint significantly. We control the resulting compression  error via a novel instance of the classicalerror feedbackmechanism from distributed optimization in whichthe error correction information is itself compressedto allow for practical memory gains. We prove that the resulting approach maintains theoretical convergence guarantees competitive to those of AMSGrad, while providing good practical performance. Specifically, we show that MicroAdam can be implemented efficiently on GPUs: on both million-scale (BERT) and billion-scale (LLaMA) models, MicroAdam provides practical convergence competitive to that of the uncompressed Adam baseline, with  lower memory usage and similar running time. Our code is available at https://github.com/IST-DASLab/MicroAdam."
"GITA: Graph to Visual and Textual Integration for Vision-Language Graph ReasoningPart ofAdvances in Neural Information Processing Systems 37  (NeurIPS 2024)Main Conference TrackBibtexPaperSupplementalAuthorsYanbin Wei, Shuai Fu, Weisen Jiang, Zejian Zhang, Zhixiong Zeng, Qi Wu, James T. Kwok, Yu ZhangAbstractLarge Language Models (LLMs) are increasingly used for various tasks with graph structures. Though LLMs can process graph information in a textual format, they overlook the rich vision modality, which is an intuitive way for humans to comprehend structural information and conduct general graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., $\textit{visual graph}$) are still unexplored. To fill the gap, we innovatively propose an end-to-end framework, called $\textbf{G}$raph to v$\textbf{I}$sual and $\textbf{T}$extual Integr$\textbf{A}$tion (GITA), which firstly incorporates visual graphs into general graph reasoning. Besides, we establish  $\textbf{G}$raph-based $\textbf{V}$ision-$\textbf{L}$anguage $\textbf{Q}$uestion $\textbf{A}$nswering (GVLQA) dataset from existing graph data, which is the first vision-language dataset for general graph reasoning purposes. Extensive experiments on the GVLQA dataset and five real-world datasets show that GITA outperforms mainstream LLMs in terms of general graph reasoning capabilities. Moreover, We highlight the effectiveness of the layout augmentation on visual graphs and pretraining on the GVLQA dataset."
"How does PDE order affect the convergence of PINNs?Part ofAdvances in Neural Information Processing Systems 37  (NeurIPS 2024)Main Conference TrackBibtexPaperSupplementalAuthorsChanghoon Song, Yesom Park, Myungjoo KangAbstractThis paper analyzes the inverse relationship between the order of partial differential equations (PDEs) and the convergence of gradient descent in physics-informed neural networks (PINNs) with the power of ReLU activation. The integration of the PDE into a loss function endows PINNs with a distinctive feature to require computing derivatives of model up to the PDE order. Although it has been empirically observed that PINNs encounter difficulties in convergence when dealing with high-order or high-dimensional PDEs, a comprehensive theoretical understanding of this issue remains elusive. This paper offers theoretical support for this pathological behavior by demonstrating that the gradient flow converges in a lower probability when the PDE order is higher. In addition, we show that PINNs struggle to address high-dimensional problems because the influence of dimensionality on convergence is exacerbated with increasing PDE order. To address the pathology, we use the insights garnered to consider variable splitting that decomposes the high-order PDE into a system of lower-order PDEs. We prove that by reducing the differential order, the gradient flow of variable splitting is more likely to converge to the global optimum. Furthermore, we present numerical experiments in support of our theoretical claims."
"Fair Wasserstein CoresetsPart ofAdvances in Neural Information Processing Systems 37  (NeurIPS 2024)Main Conference TrackBibtexPaperAuthorsZikai Xiong, Niccolò Dalmasso, Shubham Sharma, Freddy Lecue, Daniele Magazzeni, Vamsi K. Potluru, Tucker Balch, Manuela VelosoAbstractData distillation and coresets have emerged as popular approaches to generate a smaller representative set of samples for downstream learning tasks to handle large-scale datasets. At the same time, machine learning is being increasingly applied to decision-making processes at a societal level, making it imperative for modelers to address inherent biases towards subgroups present in the data. While current approaches focus on creating fair synthetic representative samples by optimizing local properties relative to the original samples, their impact on downstream learning processes has yet to be explored.  In this work, we present fair Wasserstein coresets ($\texttt{FWC}$), a novel coreset approach which generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. $\texttt{FWC}$ uses an efficient majority minimization algorithm to minimize the Wasserstein distance between the original dataset and the weighted synthetic samples while enforcing demographic parity. We show that an unconstrained version of $\texttt{FWC}$ is equivalent to Lloyd's algorithm for k-medians and k-means clustering. Experiments conducted on both synthetic and real datasets show that $\texttt{FWC}$:  (i) achieves a competitive fairness-performance tradeoff in downstream models compared to existing approaches, (ii) improves downstream fairness when added to the existing training data and (iii) can be used to reduce biases in predictions from large language models (GPT-3.5 and GPT-4)."
"Improved Regret for Bandit Convex Optimization with Delayed FeedbackPart ofAdvances in Neural Information Processing Systems 37  (NeurIPS 2024)Main Conference TrackBibtexPaperAuthorsYuanyu Wan, Chang Yao, Mingli Song, Lijun ZhangAbstractWe investigate bandit convex optimization (BCO) with delayed feedback, where only the loss value of the action is revealed under an arbitrary delay. Let $n,T,\bar{d}$ denote the dimensionality, time horizon, and average delay, respectively. Previous studies have achieved an $O(\sqrt{n}T^{3/4}+(n\bar{d})^{1/3}T^{2/3})$ regret bound for this problem, whose delay-independent part matches the regret of the classical non-delayed bandit gradient descent algorithm. However, there is a large gap between its delay-dependent part, i.e., $O((n\bar{d})^{1/3}T^{2/3})$, and an existing $\Omega(\sqrt{\bar{d}T})$ lower bound. In this paper, we illustrate that this gap can be filled in the worst case, where $\bar{d}$ is very close to the maximum delay $d$. Specifically, we first develop a novel algorithm, and prove that it enjoys a regret bound of $O(\sqrt{n}T^{3/4}+\sqrt{dT})$ in general. Compared with the previous result, our regret bound is better for $d=O((n\bar{d})^{2/3}T^{1/3})$, and the delay-dependent part is tight in the worst case. The primary idea is to decouple the joint effect of the delays and the bandit feedback on the regret by carefully incorporating the delayed bandit feedback with a blocking update mechanism. Furthermore, we show that the proposed algorithm can improve the regret bound to $O((nT)^{2/3}\log^{1/3}T+d\log T)$ for strongly convex functions. Finally, if the action sets are unconstrained, we demonstrate that it can be simply extended to achieve an $O(n\sqrt{T\log T}+d\log T)$ regret bound for strongly convex and smooth functions."
"Enhancing Chess Reinforcement Learning with Graph RepresentationPart ofAdvances in Neural Information Processing Systems 37  (NeurIPS 2024)Main Conference TrackBibtexPaperSupplementalAuthorsTomas Rigaux, Hisashi KashimaAbstractMastering games is a hard task, as games can be extremely complex, and still fundamentally different in structure from one another. While the AlphaZero algorithm has demonstrated an impressive ability to learn the rules and strategy of a large variety of games, ranging from Go and Chess, to Atari games, its reliance on extensive computational resources and rigid Convolutional Neural Network (CNN) architecture limits its adaptability and scalability. A model trained to play on a $19\times 19$ Go board cannot be used to play on a smaller $13\times 13$ board, despite the similarity between the two Go variants.In this paper, we focus on Chess, and explore using a more generic Graph-based Representation of a game state, rather than a grid-based one, to introduce a more general architecture based on Graph Neural Networks (GNN). We also expand the classical Graph Attention Network (GAT) layer to incorporate edge-features, to naturally provide a generic policy output format.Our experiments, performed on smaller networks than the initial AlphaZero paper, show that this new architecture outperforms previous architectures with a similar number of parameters, being able to increase playing strength an order of magnitude faster. We also show that the model, when trained on a smaller $5\times 5$ variant of chess, is able to be quickly fine-tuned to play on regular $8\times 8$ chess, suggesting that this approach yields promising generalization abilities.Our code is available at https://github.com/akulen/AlphaGateau."
"Mixtures of Experts for Audio-Visual LearningPart ofAdvances in Neural Information Processing Systems 37  (NeurIPS 2024)Main Conference TrackBibtexPaperSupplementalAuthorsYing Cheng, Yang Li, Junjie He, Rui FengAbstractWith the rapid development of multimedia technology, audio-visual learning has emerged as a promising research topic within the field of multimodal analysis. In this paper, we explore parameter-efficient transfer learning for audio-visual learning and propose the Audio-Visual Mixture of Experts (\ourmethodname) to inject adapters into pre-trained models flexibly. Specifically, we introduce unimodal and cross-modal adapters as multiple experts to specialize in intra-modal and inter-modal information, respectively, and employ a lightweight router to dynamically allocate the weights of each expert according to the specific demands of each task. Extensive experiments demonstrate that our proposed approach \ourmethodname achieves superior performance across multiple audio-visual tasks, including AVE, AVVP, AVS, and AVQA. Furthermore, visual-only experimental results also indicate that our approach can tackle challenging scenes where modality information is missing.The source code is available at \url{https://github.com/yingchengy/AVMOE}."
"Learning Place Cell Representations and Context-Dependent RemappingPart ofAdvances in Neural Information Processing Systems 37  (NeurIPS 2024)Main Conference TrackBibtexPaperAuthorsMarkus Pettersen, Frederik Rogge, Mikkel Elle LepperødAbstractHippocampal place cells are known for their spatially selective firing patterns, which has led to the suggestion that they encode an animal's location. However, place cells also respond to contextual cues, such as smell. Furthermore, they have the ability to remap, wherein the firing fields and rates of cells change in response to changes in the environment. How place cell responses emerge, and how these representations remap is not fully understood. In this work, we propose a similarity-based objective function that translates proximity in space, to proximity in representation. We show that a neural network trained to minimize the proposed objective learns place-like representations. We also show that the proposed objective is easily extended to include other sources of information, such as context information, in the same way. When trained to encode multiple contexts, networks learn distinct representations, exhibiting remapping behaviors between contexts. The proposed objective is invariant to orthogonal transformations. Such transformations of the original trained representation (e.g. rotations), therefore yield new representations distinct from the original, without explicit relearning, akin to remapping. Our findings shed new light on the formation and encoding properties of place cells, and also demonstrate an interesting case of representational reuse."
"Robust Sparse Regression with Non-Isotropic DesignsPart ofAdvances in Neural Information Processing Systems 37  (NeurIPS 2024)Main Conference TrackBibtexPaperAuthorsChih-Hung Liu, Gleb NovikovAbstractWe develop a technique to design efficiently computable estimators for sparse linear regression in the simultaneous presence of two adversaries: oblivious and adaptive.Consider the model $y^*=X^*\beta^*+ \eta$ where $X^*$ is an $n\times d$ random design matrix, $\beta^*\in \mathbb{R}^d$ is a $k$-sparse vector, and the noise $\eta$ is independent of $X^*$ and chosen by the \emph{oblivious adversary}. Apart from the independence of $X^*$, we only require a small fraction entries of $\eta$ to have magnitude at most $1$. The \emph{adaptive adversary} is allowed to arbitrarily corrupt an $\varepsilon$-fraction of the samples $(X_1^*, y_1^*),\ldots, (X_n^*, y_n^*)$.Given the $\varepsilon$-corrupted samples $(X_1, y_1),\ldots, (X_n, y_n)$, the goal is to estimate $\beta^*$.  We assume that the rows of $X^*$ are iid samples from some $d$-dimensional distribution $\mathcal{D}$  with zero mean and (unknown) covariance matrix $\Sigma$ with bounded condition number.We design several robust algorithms that outperform the state of the art even in the special case of Gaussian noise $\eta \sim N(0,1)^n$. In particular, we provide a polynomial-time algorithm that with high probability recovers $\beta^*$ up to error $O(\sqrt{\varepsilon})$  as long as  $n \ge \tilde{O}(k^2/\varepsilon)$, only assuming some bounds on the third and the fourth moments of $\mathcal{D}$. In addition, prior to this work, even in the special case of Gaussian design $\mathcal{D} = N(0,\Sigma)$ and noise $\eta \sim N(0,1)$, no polynomial time algorithm was known to achieve error $o(\sqrt{\varepsilon})$ in the sparse setting $n < d^2$.  We show that under some assumptions on the fourth and the eighth moments of $\mathcal{D}$, there is a polynomial-time algorithm that achieves error $o(\sqrt{\varepsilon})$ as long as $n \ge \tilde{O}(k^4 / \varepsilon^3)$. For Gaussian distribution $\mathcal{D} = N(0,\Sigma)$, this algorithm achieves error $O(\varepsilon^{3/4})$. Moreover, our algorithm achieves error $o(\sqrt{\varepsilon})$ for all log-concave distributions if $\varepsilon \le 1/\text{polylog(d)}$. Our algorithms are based on the filtering  of the covariates that uses sum-of-squares relaxations, and weighted Huber loss minimization with $\ell_1$ regularizer.  We provide a novel analysis of weighted penalized Huber loss that is suitable for heavy-tailed designs in the presence of two adversaries. Furthermore, we complement our algorithmic results with Statistical Query lower bounds, providing evidence that our estimators are likely to have nearly optimal sample complexity."



